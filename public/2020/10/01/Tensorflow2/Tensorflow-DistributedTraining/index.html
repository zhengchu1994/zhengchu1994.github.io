<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 4.2.1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-moon_32px.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-moon_32px.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-moon_16px.png?v=7.3.0">
  <link rel="mask-icon" href="/images/favicon-moon_32px.png?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="分布式训练The code here is similar to the multi-GPU training tutorial with one key difference: when using Estimator for multi-worker training, it is necessary to shard the dataset by the number of workers">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow2-DistributedTraining">
<meta property="og:url" content="http://yoursite.com/2020/10/01/Tensorflow2/Tensorflow-DistributedTraining/index.html">
<meta property="og:site_name" content="Zheng Chu&#39;s Blog">
<meta property="og:description" content="分布式训练The code here is similar to the multi-GPU training tutorial with one key difference: when using Estimator for multi-worker training, it is necessary to shard the dataset by the number of workers">
<meta property="article:published_time" content="2020-10-01T13:53:32.000Z">
<meta property="article:modified_time" content="2020-12-06T13:15:08.825Z">
<meta property="article:author" content="Zheng Chu">
<meta property="article:tag" content="Tensorflow">
<meta name="twitter:card" content="summary">
  <link rel="canonical" href="http://yoursite.com/2020/10/01/Tensorflow2/Tensorflow-DistributedTraining/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Tensorflow2-DistributedTraining | Zheng Chu's Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?<622a1a023f2bb8cdefcf61e3855bf317>";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zheng Chu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">让希望永驻</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-主页">
      
    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home  //"></i> <br>主页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-所有专栏">
      
    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th //"></i> <br>所有专栏</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-历史文章">
      
    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive //"></i> <br>历史文章</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-标签">
      
    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags  //"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-关于我">
      
    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user //"></i> <br>关于我</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    

  <a href="https://github.com/zhengchu1994" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/10/01/Tensorflow2/Tensorflow-DistributedTraining/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zheng Chu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar/jojo3.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zheng Chu's Blog">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">Tensorflow2-DistributedTraining

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-10-01 21:53:32" itemprop="dateCreated datePublished" datetime="2020-10-01T21:53:32+08:00">2020-10-01</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-12-06 21:15:08" itemprop="dateModified" datetime="2020-12-06T21:15:08+08:00">2020-12-06</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Tensorflow/" itemprop="url" rel="index"><span itemprop="name">Tensorflow</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="分布式训练"><a href="#分布式训练" class="headerlink" title="分布式训练"></a>分布式训练</h2><p>The code here is similar to the <a href="https://www.tensorflow.org/tutorials/distribute/keras" target="_blank" rel="noopener">multi-GPU training tutorial</a> with one key difference: </p><p>when using Estimator for multi-worker training, it is necessary to shard the dataset by the number of workers to ensure model convergence. （ multi-worker 模式下的分布式模式下，作为包证模型收敛的手段，数据集切割分配到多个worker上。）</p><a id="more"></a>

<p>The input data is sharded by worker index, so that each worker processes <code>1/num_workers</code> distinct portions of the dataset.（分布式中的workers用以分摊训练集，输入的数据根据给定的id分配到worker上，id指定了分配的worker。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow_datasets <span class="keyword">as</span> tfds</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tfds.disable_progress_bar()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os, json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">BUFFER_SIZE = <span class="number">10000</span></span><br><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(mode, input_context=None)</span>:</span></span><br><span class="line">  datasets, info = tfds.load(name=<span class="string">'mnist'</span>,</span><br><span class="line">                                with_info=<span class="literal">True</span>,</span><br><span class="line">                                as_supervised=<span class="literal">True</span>)</span><br><span class="line">  mnist_dataset = (datasets[<span class="string">'train'</span>] <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN <span class="keyword">else</span></span><br><span class="line">                   datasets[<span class="string">'test'</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">scale</span><span class="params">(image, label)</span>:</span></span><br><span class="line">    image = tf.cast(image, tf.float32)</span><br><span class="line">    image /= <span class="number">255</span></span><br><span class="line">    <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 若存在分布式的上下文，则对数据进行分配</span></span><br><span class="line">  <span class="keyword">if</span> input_context:</span><br><span class="line">    mnist_dataset = mnist_dataset.shard(input_context.num_input_pipelines,</span><br><span class="line">                                        input_context.input_pipeline_id)</span><br><span class="line">  <span class="keyword">return</span> mnist_dataset.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)</span><br></pre></td></tr></table></figure>
<h1 id="Multi-worker-training-with-Keras"><a href="#Multi-worker-training-with-Keras" class="headerlink" title="Multi-worker training with Keras"></a>Multi-worker training with Keras</h1><p>multi-worker distributed training with Keras model using <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy" target="_blank" rel="noopener"><code>tf.distribute.Strategy</code></a> API, specifically <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" target="_blank" rel="noopener"><code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>.（strategy是多workers模式的接口，使得单机keras模型的代码可以无缝迁移到多workers环境。）</p>
<p>准备工作：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mnist_dataset</span><span class="params">(batch_size)</span>:</span></span><br><span class="line">  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()</span><br><span class="line">  <span class="comment"># The `x` arrays are in uint8 and have values in the range [0, 255].</span></span><br><span class="line">  <span class="comment"># We need to convert them to float32 with values in the range [0, 1]</span></span><br><span class="line">  x_train = x_train / np.float32(<span class="number">255</span>)</span><br><span class="line">  y_train = y_train.astype(np.int64)</span><br><span class="line">  train_dataset = tf.data.Dataset.from_tensor_slices(</span><br><span class="line">      (x_train, y_train)).shuffle(<span class="number">60000</span>).repeat().batch(batch_size)</span><br><span class="line">  <span class="keyword">return</span> train_dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_and_compile_cnn_model</span><span class="params">()</span>:</span></span><br><span class="line">  model = tf.keras.Sequential([</span><br><span class="line">      tf.keras.Input(shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">      tf.keras.layers.Reshape(target_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)),</span><br><span class="line">      tf.keras.layers.Conv2D(<span class="number">32</span>, <span class="number">3</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">      tf.keras.layers.Flatten(),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">      tf.keras.layers.Dense(<span class="number">10</span>)</span><br><span class="line">  ])</span><br><span class="line">  model.compile(</span><br><span class="line">      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="literal">True</span>),</span><br><span class="line">      optimizer=tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>),</span><br><span class="line">      metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">  <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">per_worker_batch_size = <span class="number">64</span></span><br><span class="line">single_worker_dataset = mnist_dataset(per_worker_batch_size)</span><br><span class="line">single_worker_model = build_and_compile_cnn_model()</span><br><span class="line">single_worker_model.fit(single_worker_dataset, epochs=<span class="number">3</span>, steps_per_epoch=<span class="number">70</span>)</span><br></pre></td></tr></table></figure>
<p>TensorFlow里, <code>TF_CONFIG</code>是设置多机器训练环境变量的JSON串, 每一个机器很可能有不同的任务. </p>
<p><code>TF_CONFIG</code>由两部分组成： <code>cluster</code> 和 <code>task</code>. </p>
<p><code>cluster</code> provides information about the training cluster, which is a dict consisting of different types of jobs such as <code>worker</code>. </p>
<p>In multi-worker training with <code>MultiWorkerMirroredStrategy</code>, there is usually one <code>worker</code> that takes on a little more responsibility like <strong>saving checkpoint and writing summary file</strong> for TensorBoard in addition to what a regular <code>worker</code> does. (Chief worker执行保存checkpoint和summary的任务)</p>
<p>Such worker is referred to as the <code>chief</code> worker, and it is customary that the <code>worker</code> with <code>index</code> 0 is appointed as the chief <code>worker</code> (in fact this is how <code>tf.distribute.Strategy</code> is implemented). （chief worker 的index为0）</p>
<p><code>task</code> on the other hand provides information of the current task. （task提供当前任务的相关信息）</p>
<p>The first component <code>cluster</code> is the same for all workers, and the second component <code>task</code> is different on each worker and specifies the <code>type</code> and <code>index</code> of that worker. （cluster对于所有的workers都一样，task对不同的worker有不同的安排。）</p>
<p>In this example, we set the task <code>type</code> to <code>&quot;worker&quot;</code> and the task <code>index</code> to <code>0</code>. This means the machine that has such setting is the first worker, which will be appointed as the chief worker and do more work than other workers. （分配的task信息里标记有index=0，type=worker的机器被认为是chief。）</p>
<p>Note that other machines will need to have <code>TF_CONFIG</code> environment variable set as well, and it should have the same <code>cluster</code> dict, but different task <code>type</code> or task <code>index</code> depending on what the roles of those machines are.</p>
<p>（其他机器的角色也根据获得的task信息而做配置。）</p>
<p>For illustration purposes, this tutorial shows how one may set a <code>TF_CONFIG</code> with 2 workers on <code>localhost</code>.  In practice, users would create multiple workers on external IP addresses/ports, and set <code>TF_CONFIG</code> on each worker appropriately.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置两个workers 在localhost上。</span></span><br><span class="line">os.environ[<span class="string">'TF_CONFIG'</span>] = json.dumps(&#123;</span><br><span class="line">    <span class="string">'cluster'</span>: &#123;</span><br><span class="line">        <span class="string">'worker'</span>: [<span class="string">"localhost:12345"</span>, <span class="string">"localhost:23456"</span>]</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">'task'</span>: &#123;<span class="string">'type'</span>: <span class="string">'worker'</span>, <span class="string">'index'</span>: <span class="number">0</span>&#125;</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<h2 id="Choose-the-right-strategy"><a href="#Choose-the-right-strategy" class="headerlink" title="Choose the right strategy"></a>Choose the right strategy</h2><p>tensorflow上的分布式训练分为两种，同步与异步训练两种方式；同步训练时，所有的变量都被复制到每一个workers上，</p>
<p>In TensorFlow, distributed training consists of synchronous training, where the steps of training are synced across the workers and replicas, and asynchronous training, where the training steps are not strictly synced.（同步与异步训练两种方式）</p>
<p><code>MultiWorkerMirroredStrategy</code>, which is the recommended strategy for <strong>synchronous</strong> multi-worker training, will be demonstrated in this guide. To train the model, use an instance of <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/MultiWorkerMirroredStrategy" target="_blank" rel="noopener"><code>tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>.（该接口执行同步训练） <code>MultiWorkerMirroredStrategy</code> creates copies of all variables in the model’s layers on each device across all workers.（所有的变量都被复制到每一个workers上。）</p>
<p> It uses <code>CollectiveOps</code>, a TensorFlow op for collective communication, to aggregate gradients and keep the variables in sync. The <a href="https://www.tensorflow.org/guide/distributed_training" target="_blank" rel="noopener"><code>tf.distribute.Strategy</code> guide</a> has more details about this strategy.（它通过<code>CollectiveOps</code>操作使得变量间的梯度保持同步。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()</span><br></pre></td></tr></table></figure>
<p><strong>Note:</strong> <code>TF_CONFIG</code> is parsed and TensorFlow’s GRPC servers are started at the time <code>MultiWorkerMirroredStrategy()</code> is called, so <code>TF_CONFIG</code> environment variable must be set before a <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy" target="_blank" rel="noopener"><code>tf.distribute.Strategy</code></a> instance is created.（在<code>strategy</code>策略执行前，必须初始化<code>TF_CONFIG</code>。)</p>
<p><code>MultiWorkerMirroredStrategy</code> provides multiple implementations via the <a href="https://github.com/tensorflow/tensorflow/blob/a385a286a930601211d78530734368ccb415bee4/tensorflow/python/distribute/cross_device_ops.py#L928" target="_blank" rel="noopener"><code>CollectiveCommunication</code></a> parameter. <code>RING</code> implements ring-based collectives using gRPC as the cross-host communication layer. <code>NCCL</code> uses <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener">Nvidia’s NCCL</a> to implement collectives. <code>AUTO</code> defers the choice to the runtime. The best choice of collective implementation depends upon the number and kind of GPUs, and the network interconnect in the cluster。（<code>MultiWorkerMirroredStrategy</code>提供了多种实现方式，可以通过参数<code>CollectiveCommunication</code>设置）</p>
<h5 id="Train-the-model-with-MultiWorkerMirroredStrategy"><a href="#Train-the-model-with-MultiWorkerMirroredStrategy" class="headerlink" title="Train the model with MultiWorkerMirroredStrategy"></a>Train the model with MultiWorkerMirroredStrategy</h5><p>With the integration of <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy" target="_blank" rel="noopener"><code>tf.distribute.Strategy</code></a> API into <a href="https://www.tensorflow.org/api_docs/python/tf/keras" target="_blank" rel="noopener"><code>tf.keras</code></a>, the only change you will make to distribute the training to multi-worker is enclosing the model building and <code>model.compile()</code> call inside <code>strategy.scope()</code>. The distribution strategy’s scope dictates how and where the variables are created, and in the case of <code>MultiWorkerMirroredStrategy</code>, the variables created are <code>MirroredVariable</code>s, and they are replicated on each of the workers.（<code>strategy.scope()</code>告知了变量在哪里被什么模型所创建。）</p>
<p><strong>Note:</strong> Currently there is a limitation in <code>MultiWorkerMirroredStrategy</code> where TensorFlow ops need to be created after the instance of strategy is created. If you see <code>RuntimeError: Collective ops must be configured at program startup</code>, try creating the instance of <code>MultiWorkerMirroredStrategy</code> at the beginning of the program and put the code that may create ops after the strategy is instantiated.</p>
<p>（<code>RuntimeError: Collective ops must be configured at program startup</code> 这个bug怎么处理）</p>
<p><strong>Note:</strong> If you have an infinite dataset (by calling <code>.repeat()</code> on the dataset), you must specify the number of steps to run through <code>steps_per_epoch</code> argument to <code>model.fit()</code>. </p>
<p>In that case, <code>model.fit()</code> does not create a new iterator from the input every epoch, but continues from wherever the last epoch ended.</p>
<p> If you have a finite dataset, setting <code>steps_per_epoch</code> is optional. In particular, if the sharding is not balanced (for example, this could happen if you have a file-based dataset with the number of files more than the number of workers and some workers get files that contain more data than others. You can shard the data more evenly by manually setting <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/AutoShardPolicy" target="_blank" rel="noopener"><code>tf.data.experimental.AutoShardPolicy</code></a>, more details <a href="https://www.tensorflow.org/tutorials/distribute/input#sharding" target="_blank" rel="noopener">here</a>), and <code>steps_per_epoch</code> is not set or set to be greater than the size of the smallest shard divided by the per-worker batch size, you might get partial batches towards the end of training. （数据的需求不同，在多个worker上怎么平衡。）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">num_workers = <span class="number">4</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Here the batch size scales up by number of workers since </span></span><br><span class="line"><span class="comment"># `tf.data.Dataset.batch` expects the global batch size. Previously we used 64, </span></span><br><span class="line"><span class="comment"># and now this becomes 128.</span></span><br><span class="line">global_batch_size = per_worker_batch_size * num_workers</span><br><span class="line">multi_worker_dataset = mnist_dataset(global_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> strategy.scope():</span><br><span class="line">  <span class="comment"># Model building/compiling need to be within `strategy.scope()`.</span></span><br><span class="line">  multi_worker_model = build_and_compile_cnn_model()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Keras' `model.fit()` trains the model with specified number of epochs and</span></span><br><span class="line"><span class="comment"># number of steps per epoch. Note that the numbers here are for demonstration</span></span><br><span class="line"><span class="comment"># purposes only and may not sufficiently produce a model with good quality.</span></span><br><span class="line">multi_worker_model.fit(multi_worker_dataset, epochs=<span class="number">3</span>, steps_per_epoch=<span class="number">70</span>)</span><br></pre></td></tr></table></figure>
<p><code>fit</code>接口里<code>steps_per_epoch</code>的意思：一个<code>epoch</code>传递多少次数据，每次传递的数据大小都为batch-size。</p>
<p><code>steps_per_epoch</code>：Since Keras data generator is meant to loop infinitely, <code>steps_per_epoch</code> indicates how many times you will fetch a new batch from generator during single epoch. Therefore, if you simply take <code>steps_per_epoch = int(number_of_train_samples / batch_size)</code>, your last batch would have less than <code>batch_size</code> items and would be discarded（每一个epoch的数据量 = steps_per_epoch * batch_size）</p>
<p><code>steps_per_epoch</code> is the number of batches of your set batch size is ran through the network in one epoch.</p>
<p>You have set your <code>steps_per_epoch</code> to be <code>training_set_size//batch_size</code> for a good reason. This ensures all data are trained upon in one epoch, providing the number divides exactly (if not it rounds by the // operator).</p>
<h5 id="Dataset-sharding-and-batch-size"><a href="#Dataset-sharding-and-batch-size" class="headerlink" title="Dataset sharding and batch size"></a>Dataset sharding and batch size</h5><p>In multi-worker training with <code>MultiWorkerMirroredStrategy</code>, sharding the dataset is needed to ensure convergence and performance. However, note that in above code snippet, the datasets are directly passed to <code>model.fit()</code> without needing to shard; this is because <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/Strategy" target="_blank" rel="noopener"><code>tf.distribute.Strategy</code></a> API takes care of the dataset sharding automatically. It shards the dataset at the file level which may create skewed shards. In extreme cases where there is only one file, only the first shard (i.e. worker) will get training or evaluation data and as a result all workers will get errors. （该API已经实现了分布式下合理的数据分割）</p>
<p>If you prefer manual sharding for your training, automatic sharding can be turned off via <a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions" target="_blank" rel="noopener"><code>tf.data.experimental.DistributeOptions</code></a> api. Concretely,</p>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># parse the TF_CONFIG</span></span><br><span class="line">resolver = tf.distribute.cluster_resolver.TFConfigClusterResolver()</span><br><span class="line">cluster_spec = resolver.cluster_spec().as_dict()</span><br><span class="line"></span><br><span class="line">----------</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TFConfigClusterResolver</span><span class="params">(ClusterResolver)</span>:</span></span><br><span class="line">  <span class="string">"""Implementation of a ClusterResolver which reads the TF_CONFIG EnvVar.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is an implementation of cluster resolvers when using TF_CONFIG to set</span></span><br><span class="line"><span class="string">  information about the cluster. The cluster spec returned will be</span></span><br><span class="line"><span class="string">  initialized from the TF_CONFIG environment variable.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  An example to set TF_CONFIG is:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```Python</span></span><br><span class="line"><span class="string">    os.environ['TF_CONFIG'] = json.dumps(&#123;</span></span><br><span class="line"><span class="string">      'cluster': &#123;</span></span><br><span class="line"><span class="string">          'worker': ["localhost:12345", "localhost:23456"]</span></span><br><span class="line"><span class="string">      &#125;,</span></span><br><span class="line"><span class="string">      'task': &#123;'type': 'worker', 'index': 0&#125;</span></span><br><span class="line"><span class="string">    &#125;)</span></span><br><span class="line"><span class="string">    ```</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  However, sometimes the container orchestration framework will set TF_CONFIG</span></span><br><span class="line"><span class="string">  for you. In this case, you can just create an instance without passing in any</span></span><br><span class="line"><span class="string">  arguments. You can find an example here to let Kuburnetes set TF_CONFIG for</span></span><br><span class="line"><span class="string">  you: https://github.com/tensorflow/ecosystem/tree/master/kubernetes. Then you</span></span><br><span class="line"><span class="string">  can use it with `tf.distribute.Strategy` as:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    ```Python</span></span><br><span class="line"><span class="string">    # `TFConfigClusterResolver` is already the default one in the following</span></span><br><span class="line"><span class="string">    # strategy.</span></span><br><span class="line"><span class="string">    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(</span></span><br><span class="line"><span class="string">        cluster_resolver=TFConfigClusterResolver())</span></span><br><span class="line"><span class="string">    ```</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               task_type=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               task_id=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               rpc_layer=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               environment=None)</span>:</span></span><br><span class="line">    <span class="string">"""Creates a new TFConfigClusterResolver.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      task_type: (String, optional) Overrides the task type specified in the</span></span><br><span class="line"><span class="string">        TF_CONFIG environment variable.</span></span><br><span class="line"><span class="string">      task_id: (Integer, optional) Overrides the task index specified in the</span></span><br><span class="line"><span class="string">        TF_CONFIG environment variable.</span></span><br><span class="line"><span class="string">      rpc_layer: (String, optional) Overrides the rpc layer TensorFlow uses.</span></span><br><span class="line"><span class="string">      environment: (String, optional) Overrides the environment TensorFlow</span></span><br><span class="line"><span class="string">        operates in.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">---------</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cluster_spec</span><span class="params">(self)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a ClusterSpec based on the TF_CONFIG environment variable.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">      Returns:</span></span><br><span class="line"><span class="string">        A ClusterSpec with information from the TF_CONFIG environment variable.</span></span><br><span class="line"><span class="string">      """</span></span><br><span class="line">  tf_config = _load_tf_config()</span><br><span class="line">  <span class="keyword">if</span> <span class="string">'cluster'</span> <span class="keyword">not</span> <span class="keyword">in</span> tf_config:</span><br><span class="line">    <span class="keyword">return</span> ClusterSpec(&#123;&#125;)</span><br><span class="line">  <span class="keyword">return</span> ClusterSpec(tf_config[<span class="string">'cluster'</span>])</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line">config_pb2.RunOptions</span><br><span class="line"></span><br><span class="line">_warm_start_settings</span><br></pre></td></tr></table></figure>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/10/01/Tensorflow2/Tensorflow-Estimator/" rel="next" title="Tensorflow2-Estimator">
                  <i class="fa fa-chevron-left"></i> Tensorflow2-Estimator
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/10/01/Tensorflow2/Tensorflow-QA/" rel="prev" title="Tensorflow2-QA">
                  Tensorflow2-QA <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar/jojo3.jpg"
      alt="Zheng Chu">
  <p class="site-author-name" itemprop="name">Zheng Chu</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">91</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/zhengchu1994" title="GitHub &rarr; https://github.com/zhengchu1994" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.jianshu.com/u/d4875c485cff" title="简书 &rarr; https://www.jianshu.com/u/d4875c485cff" rel="noopener" target="_blank"><i class="fa fa-fw fa-heartbeat"></i>简书</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://blog.csdn.net/NockinOnHeavensDoor" title="CSDN &rarr; https://blog.csdn.net/NockinOnHeavensDoor" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>CSDN</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zhengchu@tju.edu.cn" title="E-Mail &rarr; mailto:zhengchu@tju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#分布式训练"><span class="nav-number">1.</span> <span class="nav-text">分布式训练</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Multi-worker-training-with-Keras"><span class="nav-number"></span> <span class="nav-text">Multi-worker training with Keras</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Choose-the-right-strategy"><span class="nav-number">1.</span> <span class="nav-text">Choose the right strategy</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Train-the-model-with-MultiWorkerMirroredStrategy"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">Train the model with MultiWorkerMirroredStrategy</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dataset-sharding-and-batch-size"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">Dataset sharding and batch size</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zheng Chu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.1</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', function() {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', function() {
    var gitalk = new Gitalk({
      clientID: '0911ce8ceab7f12409f0',
      clientSecret: '6fa693e25bfc0f98e5cc0907c97cb2fe9f54bb5e',
      repo: 'Gitalk',
      owner: 'zhengchu1994',
      admin: ['zhengchu1994'],
      id: '964fe6340a846c7ebe508dcefa1d642b',
        language: window.navigator.language || window.navigator.userLanguage,
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

</body>
</html>
