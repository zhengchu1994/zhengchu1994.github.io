<!DOCTYPE html>





<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="generator" content="Hexo 4.2.1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon-moon_32px.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-moon_32px.png?v=7.3.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-moon_16px.png?v=7.3.0">
  <link rel="mask-icon" href="/images/favicon-moon_32px.png?v=7.3.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.3.0">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.3.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    save_scroll: false,
    copycode: {"enable":false,"show_result":false,"style":"mac"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: 'Copy',
      copy_success: 'Copied',
      copy_failure: 'Copy failed'
    }
  };
</script>

  <meta name="description" content="Tensorfow2.0—RNNRNN cellInput：Vector：$\mathbf{X}{t-1}$，$\mathbf{y}{t-1}$a single layer, with a single neuron：12model &#x3D; keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape&#x3D;[None, 1])])">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorfow2复习--RNN">
<meta property="og:url" content="http://yoursite.com/2020/07/04/Tensorflow2/2020-07-04-Tensorfow2%E5%A4%8D%E4%B9%A0--RNN/index.html">
<meta property="og:site_name" content="Zheng Chu&#39;s Blog">
<meta property="og:description" content="Tensorfow2.0—RNNRNN cellInput：Vector：$\mathbf{X}{t-1}$，$\mathbf{y}{t-1}$a single layer, with a single neuron：12model &#x3D; keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape&#x3D;[None, 1])])">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerg9bbnuj30ww0ds76o.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerojiu9fj31a40fkadw.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerw6bs28j310a0dadh3.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1gges1kszb4j31180g4gof.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggesrl0lkcj312w0mmdkc.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggesx40uz3j31180os77h.jpg">
<meta property="article:published_time" content="2020-07-04T11:00:00.000Z">
<meta property="article:modified_time" content="2021-03-16T01:05:09.610Z">
<meta property="article:author" content="Zheng Chu">
<meta property="article:tag" content="DeepLearning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerg9bbnuj30ww0ds76o.jpg">
  <link rel="canonical" href="http://yoursite.com/2020/07/04/Tensorflow2/2020-07-04-Tensorfow2%E5%A4%8D%E4%B9%A0--RNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>Tensorfow2复习--RNN | Zheng Chu's Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?<622a1a023f2bb8cdefcf61e3855bf317>";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>






  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <div class="container sidebar-position-left">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Zheng Chu's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">让希望永驻</p>
      
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-主页">
      
    
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home  //"></i> <br>主页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-所有专栏">
      
    
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th //"></i> <br>所有专栏</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-历史文章">
      
    
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive //"></i> <br>历史文章</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-标签">
      
    
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags  //"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-关于我">
      
    
      
    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user //"></i> <br>关于我</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    

  <a href="https://github.com/zhengchu1994" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content page-post-detail">
            

  <div id="posts" class="posts-expand">
    

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/04/Tensorflow2/2020-07-04-Tensorfow2%E5%A4%8D%E4%B9%A0--RNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zheng Chu">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar/jojo3.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zheng Chu's Blog">
    </span>
      <header class="post-header">

        
          <h1 class="post-title" itemprop="name headline">Tensorfow2复习--RNN

            
          </h1>
        

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              
                
              

              <time title="Created: 2020-07-04 19:00:00" itemprop="dateCreated datePublished" datetime="2020-07-04T19:00:00+08:00">2020-07-04</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-03-16 09:05:09" itemprop="dateModified" datetime="2021-03-16T09:05:09+08:00">2021-03-16</time>
              </span>
            
          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/DeepLearning/" itemprop="url" rel="index"><span itemprop="name">DeepLearning</span></a></span>

                
                
              
            </span>
          

          
            <span class="post-meta-item" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Tensorfow2-0—RNN"><a href="#Tensorfow2-0—RNN" class="headerlink" title="Tensorfow2.0—RNN"></a>Tensorfow2.0—RNN</h1><h4 id="RNN-cell"><a href="#RNN-cell" class="headerlink" title="RNN cell"></a>RNN cell</h4><p>Input：Vector：$\mathbf{X}<em>{t-1}$，$\mathbf{y}</em>{t-1}$</p><p>a single layer, with a single neuron：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerg9bbnuj30ww0ds76o.jpg" alt="a single layer, with a sin‐ gle neuron"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential([ keras.layers.SimpleRNN(<span class="number">1</span>, input_shape=[<span class="literal">None</span>, <span class="number">1</span>])</span><br><span class="line">])</span><br></pre></td></tr></table></figure><a id="more"></a>



<p>By default, the SimpleRNN layer uses the <strong>hyperbolic tangent activation function.</strong> </p>
<p>the initial state <em>h</em>(init) is set to 0</p>
<p>In a simple RNN, this output is also the new state <em>h</em>0. </p>
<p>return one output per time step, you must set <code>return_sequences=True</code></p>
<h4 id="RNN-layer"><a href="#RNN-layer" class="headerlink" title="RNN layer"></a>RNN layer</h4><p>Input跟RNN cell一样，但是参数增加了，因为每个RNN cell对于$\mathbf{X}$还有$\mathbf{y}$分别有一个参数，多个RNN cell作为一层使得，有多个这样的参数。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerojiu9fj31a40fkadw.jpg" alt="image-20200704111333418"></p>
<h4 id="Memory-Cell"><a href="#Memory-Cell" class="headerlink" title="Memory Cell"></a>Memory Cell</h4><p> New output , namely <strong>hidden state</strong>: $\mathbf{h<em>{(t)}}=f(\mathbf{x</em>{t}}, \mathbf{h_{(t-1)}})$</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggerw6bs28j310a0dadh3.jpg" alt="image-20200704112055603"></p>
<h4 id="Deep-RNNs"><a href="#Deep-RNNs" class="headerlink" title="Deep RNNs"></a>Deep RNNs</h4><p>stack multiple layers of cells</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gges1kszb4j31180g4gof.jpg" alt="image-20200704112546983"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential([</span><br><span class="line">keras.layers.SimpleRNN(<span class="number">20</span>, return_sequences=<span class="literal">True</span>, input_shape=[<span class="literal">None</span>, <span class="number">1</span>]), keras.layers.SimpleRNN(<span class="number">20</span>),</span><br><span class="line">keras.layers.Dense(<span class="number">1</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="Handling-Long-Sequences"><a href="#Handling-Long-Sequences" class="headerlink" title="Handling Long Sequences"></a>Handling Long Sequences</h4><p>Leading to : Unstable Gradients Problem</p>
<blockquote>
<p>nonsaturating activation functions (e.g., ReLU) may not help as much here; in fact, they may actually lead the RNN to be even more unstable during training</p>
<p>Well, suppose Gradient Descent updates the weights in a way that increases the outputs slightly at the first time step. <strong>Because the same weights are used at every time step, the outputs at the second time step may also be slightly increased</strong>, and those at the third, and so on until the outputs explode—and a nonsaturating acti‐ vation function does not prevent that.</p>
</blockquote>
<p>Solution: </p>
<ul>
<li><p><em>Layer Normalization</em> : it is very similar to Batch Normalization, but instead of normalizing across the batch dimension, it normalizes <strong>across the features dimension.</strong></p>
</li>
<li><p>Before activation</p>
</li>
</ul>
<p>A cell must also have a <code>state_size</code> attribute and an <code>output_size</code> attribute.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LNSimpleRNNCell</span><span class="params">(keras.layers.Layer)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, units, activation=<span class="string">"tanh"</span>, **kwargs)</span>:</span></span><br><span class="line">  super().__init__(**kwargs)</span><br><span class="line">  self.state_size = units</span><br><span class="line">  self.output_size = units</span><br><span class="line">  self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,</span><br><span class="line">  activation=<span class="literal">None</span>) self.layer_norm = keras.layers.LayerNormalization()</span><br><span class="line">  self.activation = keras.activations.get(activation) </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, states)</span>:</span></span><br><span class="line">   <span class="comment">#new_states[0] equal to outputs</span></span><br><span class="line">	outputs, new_states = self.simple_rnn_cell(inputs, states) </span><br><span class="line">  norm_outputs = self.activation(self.layer_norm(outputs)) <span class="keyword">return</span> norm_outputs, [norm_outputs]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential([ keras.layers.RNN(LNSimpleRNNCell(<span class="number">20</span>),</span><br><span class="line">                                                   return_sequences=<span class="literal">True</span>, </span><br><span class="line">                                                   input_shape=[<span class="literal">None</span>, <span class="number">1</span>]),</span><br><span class="line">                                 keras.layers.RNN(LNSimpleRNNCell(<span class="number">20</span>),</span><br><span class="line">                                                  return_sequences=<span class="literal">True</span>),</span><br><span class="line">                                 keras.layers.TimeDistributed(keras.layers.Dense(<span class="number">10</span>)) ])</span><br></pre></td></tr></table></figure>
<ul>
<li>all recurrent layers (except for keras.layers.RNN) and all cells provided by Keras have a <strong>dropout</strong> hyperparameter and a <strong>recurrent_dropout</strong> hyperparameter: the former defines the dropout rate to apply <strong>to the inputs</strong> (at each time step), and the latter defines the dropout rate for <strong>the hidden states</strong></li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul>
<li>its state is split into two vectors: <strong>h</strong>(<em>t</em>)（short-term state） and <strong>c</strong>(<em>t</em>) （long-term state）</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggesrl0lkcj312w0mmdkc.jpg" alt="image-20200704115106939"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = keras.models.Sequential([</span><br><span class="line">  keras.layers.LSTM(<span class="number">20</span>, return_sequences=<span class="literal">True</span>, input_shape=[<span class="literal">None</span>, <span class="number">1</span>]),</span><br><span class="line">  keras.layers.LSTM(<span class="number">20</span>, return_sequences=<span class="literal">True</span>),</span><br><span class="line">  keras.layers.TimeDistributed(keras.layers.Dense(<span class="number">10</span>))</span><br><span class="line">])</span><br></pre></td></tr></table></figure>
<h4 id="GRU-cells"><a href="#GRU-cells" class="headerlink" title="GRU cells"></a>GRU cells</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggesx40uz3j31180os77h.jpg" alt="image-20200704115624823"></p>
<h4 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h4><h2 id="2021-03-更新"><a href="#2021-03-更新" class="headerlink" title="2021-03 更新"></a>2021-03 更新</h2><h3 id="TF1"><a href="#TF1" class="headerlink" title="TF1"></a>TF1</h3><h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><p>输入<code>X</code>的长度：</p>
<blockquote>
<p>X:  Tensor(“embedding_lookup/Identity:0”, shape=(1, 75, 100), dtype=float32)   <strong>[批量大小， 点击序列长度，每个点击的Embedding长度]</strong></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(<span class="string">"bilstm"</span>, reuse=reuse):</span><br><span class="line">  forward_output, _ = tf.nn.dynamic_rnn(</span><br><span class="line">    tf.contrib.rnn.LSTMCell(self.hidden_size[<span class="number">0</span>],</span><br><span class="line">                            initializer=self.initializer,</span><br><span class="line">                            reuse=tf.AUTO_REUSE),</span><br><span class="line">    X,</span><br><span class="line">    dtype=tf.float32,</span><br><span class="line">    sequence_length=length, <span class="comment"># length = 75 , 也就是点击序列的长度</span></span><br><span class="line">    scope=<span class="string">"RNN_forward"</span>)</span><br></pre></td></tr></table></figure>
<p><a href="https://zhuanlan.zhihu.com/p/43041436" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43041436</a></p>
<p><code>LSTMCell</code>：</p>
<blockquote>
<p><strong>init</strong> (</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">num_units,</span><br><span class="line"></span><br><span class="line">use_peepholes=<span class="literal">False</span>,</span><br><span class="line"></span><br><span class="line">cell_clip=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">initializer=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">num_proj=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">proj_clip=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">num_unit_shards=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">num_proj_shards=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">forget_bias=<span class="number">1.0</span>,</span><br><span class="line"></span><br><span class="line">state_is_tuple=<span class="literal">True</span>,</span><br><span class="line"></span><br><span class="line">activation=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">reuse=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">name=<span class="literal">None</span>,</span><br><span class="line"></span><br><span class="line">dtype=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>参数说明：</p>
<p>num_units:LSTM cell中的单元数量，即隐藏层神经元数量。<br>use_peepholes:布尔类型，设置为True则能够使用peephole连接<br>cell_clip:可选参数，float类型，如果提供，则在单元输出激活之前，通过该值裁剪单元状态。<br>Initializer:可选参数，用于权重和投影矩阵的初始化器。<br>num_proj:可选参数，int类型，投影矩阵的输出维数，如果为None，则不执行投影。<br>pro_clip:可选参数，float型，如果提供了num_proj&gt;0和proj_clip，则投影值将元素裁剪到[-proj_clip,proj_clip]范围。<br>num_unit_shards:弃用。<br>num_proj_shards:弃用。<br>forget_bias:float类型，偏置增加了忘记门。从CudnnLSTM训练的检查点(checkpoin)恢复时，必须手动设置为0.0。<br>state_is_tuple:如果为True，则接受和返回的状态是c_state和m_state的2-tuple；如果为False，则他们沿着列轴连接。后一种即将被弃用。<br>activation:内部状态的激活函数。默认为tanh<br>reuse:布尔类型，描述是否在现有范围中重用变量。如果不为True，并且现有范围已经具有给定变量，则会引发错误。<br>name:String类型，层的名称。具有相同名称的层将共享权重，但为了避免错误，在这种情况下需要reuse=True.<br>dtype:该层默认的数据类型。默认值为None表示使用第一个输入的类型。在call之前build被调用则需要该参数。<br>————————————————<br>版权声明：本文为CSDN博主「大雄没有叮当猫」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/u013230189/article/details/82811066" target="_blank" rel="noopener">https://blog.csdn.net/u013230189/article/details/82811066</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">embedding_dim = <span class="number">300</span></span><br><span class="line"></span><br><span class="line">inputs = tf.Variable(tf.random_normal([batch_size, embedding_dim]))</span><br><span class="line"></span><br><span class="line">previous_state = (tf.Variable(tf.random_normal([batch_size, <span class="number">100</span>])), tf.Variable(tf.random_normal([batch_size, <span class="number">100</span>])))</span><br><span class="line"></span><br><span class="line">lstmcell = tf.nn.rnn_cell.LSTMCell(<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">outputs, (h_state, c_state) = lstmcell(inputs, previous_state)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line">print(sess.run(outputs))</span><br><span class="line">print(outputs.shape)  <span class="comment"># (10, 128)</span></span><br><span class="line"></span><br><span class="line">print(h_state.shape)  <span class="comment"># (10, 128)</span></span><br><span class="line"></span><br><span class="line">print(c_state.shape)  <span class="comment"># (10, 128)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">[[<span class="number">-0.03908409</span> <span class="number">-0.15413918</span> <span class="number">-0.5543078</span>  <span class="number">-0.16429745</span>  <span class="number">0.15187332</span>  <span class="number">0.22674957</span></span><br><span class="line">   <span class="number">0.0491709</span>   <span class="number">0.05772216</span> <span class="number">-0.44460103</span> <span class="number">-0.4685788</span>  <span class="number">-0.20345497</span> <span class="number">-0.3048153</span></span><br><span class="line">   <span class="number">0.08147392</span> <span class="number">-0.3003875</span>   <span class="number">0.1934505</span>  <span class="number">-0.2632878</span>  <span class="number">-0.4045717</span>  <span class="number">-0.27238455</span></span><br><span class="line">   <span class="number">0.5274059</span>   <span class="number">0.3149494</span>  <span class="number">-0.04591073</span>  <span class="number">0.2029352</span>  <span class="number">-0.7432734</span>   <span class="number">0.24162863</span></span><br><span class="line">   <span class="number">0.02106089</span> <span class="number">-0.05926758</span> <span class="number">-0.38088006</span>  <span class="number">0.00200358</span> <span class="number">-0.17426118</span>  <span class="number">0.02138741</span></span><br><span class="line">   <span class="number">0.13410263</span> <span class="number">-0.7480866</span>   <span class="number">0.59177715</span> <span class="number">-0.16158845</span>  <span class="number">0.10526363</span> <span class="number">-0.43394142</span></span><br><span class="line">  <span class="number">-0.11014693</span> <span class="number">-0.02479873</span>  <span class="number">0.12916292</span>  <span class="number">0.3426005</span>   <span class="number">0.3468578</span>   <span class="number">0.03081424</span></span><br><span class="line">  <span class="number">-0.5923045</span>  <span class="number">-0.73410743</span> <span class="number">-0.3768449</span>   <span class="number">0.18405321</span> <span class="number">-0.35003117</span> <span class="number">-0.04348066</span></span><br><span class="line">   <span class="number">0.37911254</span> <span class="number">-0.35261196</span> <span class="number">-0.21207377</span>  <span class="number">0.5164869</span>   <span class="number">0.09950166</span> <span class="number">-0.02072151</span></span><br><span class="line">  <span class="number">-0.32580587</span> <span class="number">-0.20204493</span>  <span class="number">0.04182163</span> <span class="number">-0.551953</span>    <span class="number">0.5776422</span>  <span class="number">-0.15258075</span></span><br><span class="line">  <span class="number">-0.19567099</span> <span class="number">-0.46144682</span>  <span class="number">0.10801785</span>  <span class="number">0.2929367</span>   <span class="number">0.3800717</span>  <span class="number">-0.10385328</span></span><br><span class="line">  <span class="number">-0.23831426</span> <span class="number">-0.27831694</span> <span class="number">-0.26867956</span> <span class="number">-0.52392566</span> <span class="number">-0.35053068</span> <span class="number">-0.12362379</span></span><br><span class="line">  <span class="number">-0.47640064</span>  <span class="number">0.29312813</span> <span class="number">-0.63410735</span>  <span class="number">0.27830732</span> <span class="number">-0.00418854</span> <span class="number">-0.06247476</span></span><br><span class="line">   <span class="number">0.38740724</span>  <span class="number">0.12090401</span> <span class="number">-0.34354135</span>  <span class="number">0.26057196</span> <span class="number">-0.3492871</span>   <span class="number">0.28602415</span></span><br><span class="line">  <span class="number">-0.32755297</span> <span class="number">-0.48350778</span>  <span class="number">0.03378379</span> <span class="number">-0.45831716</span> <span class="number">-0.33049983</span>  <span class="number">0.5797502</span></span><br><span class="line">  <span class="number">-0.57983845</span> <span class="number">-0.5102703</span>  <span class="number">-0.05812724</span> <span class="number">-0.06680895</span>  <span class="number">0.10354684</span> <span class="number">-0.22407153</span></span><br><span class="line">  <span class="number">-0.00116582</span> <span class="number">-0.17466179</span> <span class="number">-0.24506229</span>  <span class="number">0.40669897</span>]]</span><br><span class="line">(<span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">(<span class="number">1</span>, <span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<p><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p>
<p> <code>dynamic RNN</code>：</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.dynamic_rnn(</span><br><span class="line">    cell,</span><br><span class="line">    inputs,</span><br><span class="line">    sequence_length=<span class="literal">None</span>,</span><br><span class="line">    initial_state=<span class="literal">None</span>,</span><br><span class="line">    dtype=<span class="literal">None</span>,</span><br><span class="line">    parallel_iterations=<span class="literal">None</span>,</span><br><span class="line">    swap_memory=<span class="literal">False</span>,</span><br><span class="line">    time_major=<span class="literal">False</span>,</span><br><span class="line">    scope=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
</blockquote>
<p>具体解释：<a href="https://zhuanlan.zhihu.com/p/43041436" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/43041436</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 创建输入数据</span></span><br><span class="line">X = np.random.randn(<span class="number">2</span>, <span class="number">10</span>, <span class="number">8</span>)   <span class="comment"># batch-size, 序列最大有效长度，每个序列的embedding大小</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个example长度为6</span></span><br><span class="line">X[<span class="number">1</span>,<span class="number">6</span>:] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">"第二个样例:"</span>, X[<span class="number">1</span>])</span><br><span class="line">X_lengths = [<span class="number">10</span>, <span class="number">6</span>]  <span class="comment"># 第一个序列的有效长度为10，第二个序列的有效长度是6，超过6步的outputs，是直接被设置成0了，而last_states将7-10步的输出重复第6步的输出。可见节省了不少的计算开销</span></span><br><span class="line"></span><br><span class="line">cell = tf.contrib.rnn.BasicLSTMCell(num_units=<span class="number">64</span>, state_is_tuple=<span class="literal">True</span>) <span class="comment"># 输出最后一维的大小等于num_units</span></span><br><span class="line"></span><br><span class="line">outputs, last_states = tf.nn.dynamic_rnn(</span><br><span class="line">    cell=cell,</span><br><span class="line">    dtype=tf.float64,</span><br><span class="line">    sequence_length=X_lengths,</span><br><span class="line">    inputs=X)</span><br><span class="line"></span><br><span class="line">result = tf.contrib.learn.run_n(</span><br><span class="line">    &#123;<span class="string">"outputs"</span>: outputs, <span class="string">"last_states"</span>: last_states&#125;,</span><br><span class="line">    n=<span class="number">1</span>,</span><br><span class="line">    feed_dict=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"outputs shape: "</span>, result[<span class="number">0</span>][<span class="string">'outputs'</span>].shape)</span><br><span class="line">print(<span class="string">"第二个样例的输出: "</span>, result[<span class="number">0</span>][<span class="string">'outputs'</span>][<span class="number">1</span>].shape)</span><br><span class="line"><span class="keyword">for</span> it <span class="keyword">in</span> result[<span class="number">0</span>][<span class="string">'outputs'</span>][<span class="number">1</span>]:</span><br><span class="line">    print(it)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"last_states: "</span>, result[<span class="number">0</span>][<span class="string">'last_states'</span>], result[<span class="number">0</span>][<span class="string">'last_states'</span>])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> result[<span class="number">0</span>][<span class="string">"outputs"</span>].shape == (<span class="number">2</span>, <span class="number">10</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个example中的outputs超过6步(7-10步)的值应该为0</span></span><br><span class="line"><span class="keyword">assert</span> (result[<span class="number">0</span>][<span class="string">"outputs"</span>][<span class="number">1</span>,<span class="number">7</span>,:] == np.zeros(cell.output_size)).all()</span><br></pre></td></tr></table></figure>
<p><a href="https://blog.csdn.net/u010223750/article/details/71079036" target="_blank" rel="noopener">https://blog.csdn.net/u010223750/article/details/71079036</a></p>
<h2 id="RNN相关"><a href="#RNN相关" class="headerlink" title="RNN相关"></a>RNN相关</h2><p>1、完全图解RNN、RNN变体、Seq2Seq、Attention机制 <a href="https://zhuanlan.zhihu.com/p/28054589" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28054589</a></p>
<h3 id="TF2"><a href="#TF2" class="headerlink" title="TF2"></a>TF2</h3><p><code>keras.layers.RNN</code>：</p>
<blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&gt;@deprecation.deprecated(</span><br><span class="line">   <span class="literal">None</span>,</span><br><span class="line">   <span class="string">"Please use `keras.layers.RNN(cell)`, which is equivalent to this API"</span>)</span><br><span class="line">&gt;@tf_export(v1=[<span class="string">"nn.dynamic_rnn"</span>])</span><br><span class="line">&gt;@dispatch.add_dispatch_support</span><br><span class="line">&gt;<span class="function"><span class="keyword">def</span> <span class="title">dynamic_rnn</span><span class="params">(cell,</span></span></span><br><span class="line"><span class="function"><span class="params">               inputs,</span></span></span><br><span class="line"><span class="function"><span class="params">               sequence_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               initial_state=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               dtype=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               parallel_iterations=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               swap_memory=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               time_major=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br></pre></td></tr></table></figure>
</blockquote>
<h4 id="tf-keras-layers-LSTMCell"><a href="#tf-keras-layers-LSTMCell" class="headerlink" title="tf.keras.layers.LSTMCell"></a>tf.keras.layers.LSTMCell</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.LSTMCell(</span><br><span class="line">    units, activation=<span class="string">'tanh'</span>, recurrent_activation=<span class="string">'sigmoid'</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>, kernel_initializer=<span class="string">'glorot_uniform'</span>,</span><br><span class="line">    recurrent_initializer=<span class="string">'orthogonal'</span>,</span><br><span class="line">    bias_initializer=<span class="string">'zeros'</span>, unit_forget_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>, recurrent_regularizer=<span class="literal">None</span>, bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    kernel_constraint=<span class="literal">None</span>, recurrent_constraint=<span class="literal">None</span>, bias_constraint=<span class="literal">None</span>,</span><br><span class="line">    dropout=<span class="number">0.0</span>, recurrent_dropout=<span class="number">0.0</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Arguments</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>units</code></td>
<td>Positive integer, dimensionality of the output space.</td>
</tr>
<tr>
<td style="text-align:left"><code>activation</code></td>
<td>Activation function to use. Default: hyperbolic tangent (<code>tanh</code>). If you pass <code>None</code>, no activation is applied (ie. “linear” activation: <code>a(x) = x</code>).</td>
</tr>
<tr>
<td style="text-align:left"><code>recurrent_activation</code></td>
<td>Activation function to use for the recurrent step. Default: sigmoid (<code>sigmoid</code>). If you pass <code>None</code>, no activation is applied (ie. “linear” activation: <code>a(x) = x</code>).</td>
</tr>
<tr>
<td style="text-align:left"><code>use_bias</code></td>
<td>Boolean, (default <code>True</code>), whether the layer uses a bias vector.</td>
</tr>
<tr>
<td style="text-align:left"><code>kernel_initializer</code></td>
<td>Initializer for the <code>kernel</code> weights matrix, used for the linear transformation of the inputs. Default: <code>glorot_uniform</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>recurrent_initializer</code></td>
<td>Initializer for the <code>recurrent_kernel</code> weights matrix, used for the linear transformation of the recurrent state. Default: <code>orthogonal</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>bias_initializer</code></td>
<td>Initializer for the bias vector. Default: <code>zeros</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>unit_forget_bias</code></td>
<td>Boolean (default <code>True</code>). If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force <code>bias_initializer=&quot;zeros&quot;</code>. This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf" target="_blank" rel="noopener">Jozefowicz et al.</a></td>
</tr>
<tr>
<td style="text-align:left"><code>kernel_regularizer</code></td>
<td>Regularizer function applied to the <code>kernel</code> weights matrix. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>recurrent_regularizer</code></td>
<td>Regularizer function applied to the <code>recurrent_kernel</code> weights matrix. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>bias_regularizer</code></td>
<td>Regularizer function applied to the bias vector. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>kernel_constraint</code></td>
<td>Constraint function applied to the <code>kernel</code> weights matrix. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>recurrent_constraint</code></td>
<td>Constraint function applied to the <code>recurrent_kernel</code> weights matrix. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>bias_constraint</code></td>
<td>Constraint function applied to the bias vector. Default: <code>None</code>.</td>
</tr>
<tr>
<td style="text-align:left"><code>dropout</code></td>
<td>Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.</td>
</tr>
<tr>
<td style="text-align:left"><code>recurrent_dropout</code></td>
<td>Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.</td>
</tr>
</tbody>
</table>
</div>
<h4 id="Call-arguments"><a href="#Call-arguments" class="headerlink" title="Call arguments:"></a>Call arguments:</h4><ul>
<li><strong><code>inputs</code></strong>: A 2D tensor, with shape of <code>[batch, feature]</code>.</li>
<li><strong><code>states</code></strong>: List of 2 tensors that corresponding to the cell’s units. Both of them have shape <code>[batch, units]</code>, the first tensor is the memory state from previous time step, the second tensor is the carry state from previous time step. For timestep 0, the initial state provided by user will be feed to cell.</li>
<li><strong><code>training</code></strong>: Python boolean indicating whether the layer should behave in training mode or in inference mode. Only relevant when <code>dropout</code> or <code>recurrent_dropout</code> is used.</li>
</ul>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell</a></p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.layers.RNN(</span><br><span class="line">    cell, return_sequences=<span class="literal">False</span>, return_state=<span class="literal">False</span>, go_backwards=<span class="literal">False</span>,</span><br><span class="line">    stateful=<span class="literal">False</span>, unroll=<span class="literal">False</span>, time_major=<span class="literal">False</span>, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">Arguments</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><code>cell</code></td>
<td>A RNN cell instance or a list of RNN cell instances. A RNN cell is a class that has:A <code>call(input_at_t, states_at_t)</code> method, returning <code>(output_at_t, states_at_t_plus_1)</code>. The call method of the cell can also take the optional argument <code>constants</code>, see section “Note on passing external constants” below.A <code>state_size</code> attribute. This can be a single integer (single state) in which case it is the size of the recurrent state. This can also be a list/tuple of integers (one size per state). The <code>state_size</code> can also be TensorShape or tuple/list of TensorShape, to represent high dimension state.A <code>output_size</code> attribute. This can be a single integer or a TensorShape, which represent the shape of the output. For backward compatible reason, if this attribute is not available for the cell, the value will be inferred by the first element of the <code>state_size</code>.A <code>get_initial_state(inputs=None, batch_size=None, dtype=None)</code> method that creates a tensor meant to be fed to <code>call()</code> as the initial state, if the user didn’t specify any initial state via other means. The returned initial state should have a shape of [batch_size, cell.state_size]. The cell might choose to create a tensor full of zeros, or full of other values based on the cell’s implementation. <code>inputs</code> is the input tensor to the RNN layer, which should contain the batch size as its shape[0], and also dtype. Note that the shape[0] might be <code>None</code> during the graph construction. Either the <code>inputs</code> or the pair of <code>batch_size</code> and <code>dtype</code> are provided. <code>batch_size</code> is a scalar tensor that represents the batch size of the inputs. <code>dtype</code> is <a href="https://www.tensorflow.org/api_docs/python/tf/dtypes/DType" target="_blank" rel="noopener"><code>tf.DType</code></a> that represents the dtype of the inputs. For backward compatible reason, if this method is not implemented by the cell, the RNN layer will create a zero filled tensor with the size of [batch_size, cell.state_size]. In the case that <code>cell</code> is a list of RNN cell instances, the cells will be stacked on top of each other in the RNN, resulting in an efficient stacked RNN.</td>
</tr>
<tr>
<td style="text-align:left"><code>return_sequences</code></td>
<td>Boolean (default <code>False</code>). Whether to return the last output in the output sequence, or the full sequence.</td>
</tr>
<tr>
<td style="text-align:left"><code>return_state</code></td>
<td>Boolean (default <code>False</code>). Whether to return the last state in addition to the output.</td>
</tr>
<tr>
<td style="text-align:left"><code>go_backwards</code></td>
<td>Boolean (default <code>False</code>). If True, process the input sequence backwards and return the reversed sequence.</td>
</tr>
<tr>
<td style="text-align:left"><code>stateful</code></td>
<td>Boolean (default <code>False</code>). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch.</td>
</tr>
<tr>
<td style="text-align:left"><code>unroll</code></td>
<td>Boolean (default <code>False</code>). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.</td>
</tr>
<tr>
<td style="text-align:left"><code>time_major</code></td>
<td>The shape format of the <code>inputs</code> and <code>outputs</code> tensors. If True, the inputs and outputs will be in shape <code>(timesteps, batch, ...)</code>, whereas in the False case, it will be <code>(batch, timesteps, ...)</code>. Using <code>time_major = True</code> is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.</td>
</tr>
<tr>
<td style="text-align:left"><code>zero_output_for_mask</code></td>
<td>Boolean (default <code>False</code>). Whether the output should use zeros for the masked timesteps. Note that this field is only used when <code>return_sequences</code> is True and mask is provided. It can useful if you want to reuse the raw output sequence of the RNN without interference from the masked timesteps, eg, merging bidirectional RNNs.</td>
</tr>
</tbody>
</table>
</div>
<h4 id="ValueError-Could-not-find-matching-function-to-call-loaded-from-the-SavedModel-Got-Positional-arguments-5-total"><a href="#ValueError-Could-not-find-matching-function-to-call-loaded-from-the-SavedModel-Got-Positional-arguments-5-total" class="headerlink" title="ValueError: Could not find matching function to call loaded from the SavedModel. Got:Positional arguments (5 total):"></a>ValueError: Could not find matching function to call loaded from the SavedModel. Got:Positional arguments (5 total):</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  ValueError: Could not find matching function to call loaded from the SavedModel. Got:</span><br><span class="line">  Positional arguments (5 total):</span><br><span class="line">    * Tensor(&quot;inputs:0&quot;, shape&#x3D;(None, 75, 100), dtype&#x3D;float32)</span><br><span class="line">    * None</span><br><span class="line">    * True</span><br><span class="line">    * None</span><br><span class="line">    * None</span><br><span class="line">  Keyword arguments: &#123;&#125;</span><br><span class="line"></span><br><span class="line">Expected these arguments to match one of the following 4 option(s):</span><br><span class="line"></span><br><span class="line">Option 1:</span><br><span class="line">  Positional arguments (5 total):</span><br><span class="line">    * TensorSpec(shape&#x3D;(None, 75, 100), dtype&#x3D;tf.float32, name&#x3D;&#39;inputs&#39;)</span><br><span class="line">    * TensorSpec(shape&#x3D;(None, 75), dtype&#x3D;tf.bool, name&#x3D;&#39;mask&#39;)</span><br><span class="line">    * True</span><br><span class="line">    * None</span><br><span class="line">    * None</span><br><span class="line">  Keyword arguments: &#123;&#125;</span><br><span class="line"></span><br><span class="line">Option 2:</span><br><span class="line">  Positional arguments (5 total):</span><br><span class="line">    * TensorSpec(shape&#x3D;(None, 75, 100), dtype&#x3D;tf.float32, name&#x3D;&#39;inputs&#39;)</span><br><span class="line">    * TensorSpec(shape&#x3D;(None, 75), dtype&#x3D;tf.bool, name&#x3D;&#39;mask&#39;)</span><br><span class="line">    * False</span><br><span class="line">    * None</span><br><span class="line">    * None</span><br><span class="line">  Keyword arguments: &#123;&#125;</span><br><span class="line"></span><br><span class="line">Option 3:</span><br><span class="line">  Positional arguments (5 total):</span><br><span class="line">    * [TensorSpec(shape&#x3D;(None, None, 100), dtype&#x3D;tf.float32, name&#x3D;&#39;inputs&#x2F;0&#39;)]</span><br><span class="line">    * None</span><br><span class="line">    * False</span><br><span class="line">    * None</span><br><span class="line">    * None</span><br><span class="line">  Keyword arguments: &#123;&#125;</span><br><span class="line"></span><br><span class="line">Option 4:</span><br><span class="line">  Positional arguments (5 total):</span><br><span class="line">    * [TensorSpec(shape&#x3D;(None, None, 100), dtype&#x3D;tf.float32, name&#x3D;&#39;inputs&#x2F;0&#39;)]</span><br><span class="line">    * None</span><br><span class="line">    * True</span><br><span class="line">    * None</span><br><span class="line">    * None</span><br><span class="line">  Keyword arguments: &#123;&#125;</span><br><span class="line"></span><br><span class="line">​</span><br></pre></td></tr></table></figure>
<p>错误原因是tf2.1版本的bug：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">inputs = tf.keras.Input(shape=([<span class="number">75</span>]), dtype=<span class="string">'float32'</span>, name=<span class="string">"input_wv"</span>) </span><br><span class="line"><span class="comment"># mask_zero=True带上的话，保存为pb后不能加载，类型错误</span></span><br><span class="line">word_vectors = tf.keras.layers.Embedding(input_dim=word_num, output_dim=embedding_size, name=<span class="string">'embedding_layer'</span>,</span><br><span class="line">                                         trainable=<span class="literal">True</span>, mask_zero=<span class="literal">True</span>, weights=[w2v])(inputs)</span><br><span class="line"><span class="comment"># mask_word_vectors = tf.keras.layers.Masking(mask_value=0)(word_vectors)</span></span><br><span class="line">forward_output = tf.keras.layers.RNN(</span><br><span class="line">    tf.keras.layers.LSTMCell(<span class="number">100</span>, kernel_initializer=keras.initializers.Orthogonal(),</span><br><span class="line">                             recurrent_initializer=keras.initializers.Orthogonal()),</span><br><span class="line">    return_sequences=<span class="literal">True</span>,</span><br><span class="line">    return_state=<span class="literal">False</span>,</span><br><span class="line">    stateful=<span class="literal">False</span>,</span><br><span class="line">)(word_vectors)</span><br><span class="line">model = tf.keras.Model(inputs=[inputs], outputs=[word_vectors, forward_output], name=<span class="string">"LSTMAttn"</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>
<p>解决办法：升级tf2.2。</p>

    </div>

    
    
    
        
      

      <footer class="post-footer">
          
            
          
          <div class="post-tags">
            
              <a href="/tags/DeepLearning/" rel="tag"># DeepLearning</a>
            
          </div>
        

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/06/30/PythonTrick/" rel="next" title="Python-Trick">
                  <i class="fa fa-chevron-left"></i> Python-Trick
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/07/04/Tensorflow2/2020-07-04-Tensorfow2%E5%A4%8D%E4%B9%A0--LowerAPI/" rel="prev" title="Tensorfow2复习--LowerAPI">
                  Tensorfow2复习--LowerAPI <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="gitalk-container"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">

          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/avatar/jojo3.jpg"
      alt="Zheng Chu">
  <p class="site-author-name" itemprop="name">Zheng Chu</p>
  <div class="site-description motion-element" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives">
        
          <span class="site-state-item-count">90</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">categories</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">tags</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/zhengchu1994" title="GitHub &rarr; https://github.com/zhengchu1994" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://www.jianshu.com/u/d4875c485cff" title="简书 &rarr; https://www.jianshu.com/u/d4875c485cff" rel="noopener" target="_blank"><i class="fa fa-fw fa-heartbeat"></i>简书</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://blog.csdn.net/NockinOnHeavensDoor" title="CSDN &rarr; https://blog.csdn.net/NockinOnHeavensDoor" rel="noopener" target="_blank"><i class="fa fa-fw fa-globe"></i>CSDN</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="mailto:zhengchu@tju.edu.cn" title="E-Mail &rarr; mailto:zhengchu@tju.edu.cn" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
    
  </div>



        </div>
      </div>
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorfow2-0—RNN"><span class="nav-number">1.</span> <span class="nav-text">Tensorfow2.0—RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN-cell"><span class="nav-number">1.0.0.1.</span> <span class="nav-text">RNN cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN-layer"><span class="nav-number">1.0.0.2.</span> <span class="nav-text">RNN layer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Memory-Cell"><span class="nav-number">1.0.0.3.</span> <span class="nav-text">Memory Cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-RNNs"><span class="nav-number">1.0.0.4.</span> <span class="nav-text">Deep RNNs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Handling-Long-Sequences"><span class="nav-number">1.0.0.5.</span> <span class="nav-text">Handling Long Sequences</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM"><span class="nav-number">1.0.0.6.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU-cells"><span class="nav-number">1.0.0.7.</span> <span class="nav-text">GRU cells</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#WaveNet"><span class="nav-number">1.0.0.8.</span> <span class="nav-text">WaveNet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2021-03-更新"><span class="nav-number">1.1.</span> <span class="nav-text">2021-03 更新</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TF1"><span class="nav-number">1.1.1.</span> <span class="nav-text">TF1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#循环神经网络"><span class="nav-number">1.1.2.</span> <span class="nav-text">循环神经网络</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN相关"><span class="nav-number">1.2.</span> <span class="nav-text">RNN相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TF2"><span class="nav-number">1.2.1.</span> <span class="nav-text">TF2</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#tf-keras-layers-LSTMCell"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">tf.keras.layers.LSTMCell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Call-arguments"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">Call arguments:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">RNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ValueError-Could-not-find-matching-function-to-call-loaded-from-the-SavedModel-Got-Positional-arguments-5-total"><span class="nav-number">1.2.1.4.</span> <span class="nav-text">ValueError: Could not find matching function to call loaded from the SavedModel. Got:Positional arguments (5 total):</span></a></li></ol></li></ol></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zheng Chu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.1</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.3.0</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="Total Visitors">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="Total Views">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>








        
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>

    

  </div>

  
  <script src="/lib/jquery/index.js?v=3.4.1"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

<script src="/js/utils.js?v=7.3.0"></script>
  <script src="/js/motion.js?v=7.3.0"></script>


  <script src="/js/affix.js?v=7.3.0"></script>
  <script src="/js/schemes/pisces.js?v=7.3.0"></script>


<script src="/js/next-boot.js?v=7.3.0"></script>




  




























  

  
    
      
<script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script>
<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', function() {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script>

    
  

  

  


  
  <script src="/js/scrollspy.js?v=7.3.0"></script>
<script src="/js/post-details.js?v=7.3.0"></script>


    
<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', function() {
    var gitalk = new Gitalk({
      clientID: '0911ce8ceab7f12409f0',
      clientSecret: '6fa693e25bfc0f98e5cc0907c97cb2fe9f54bb5e',
      repo: 'Gitalk',
      owner: 'zhengchu1994',
      admin: ['zhengchu1994'],
      id: 'c73d6505e2964c6cc8b56419ca9541f4',
        language: window.navigator.language || window.navigator.userLanguage,
      
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script>

</body>
</html>
