---
title: 优化算法
mathjax: true
date: 2020-06-16 08:21:58
tags: MachineLearning
categories: 机器学习
---





This：2020.06.16

last：date: 2019-08-17 08:21:58

### 梯度如何下降                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          



一、先抓一部分数学分析的知识

（1）方向导数的出场：偏导数刻画了函数在特定坐标轴方向上的变化率，而方向导数设法获取函数在特定方向上的变化率。

★ 既然是变化率，由极限定义斜率可以猜测，这也是一个极限值；即然是极限值，那么假设在目标点$P_0$附近存在一个微小变化，导致该变化率突然增大或减小，可以想像这个地方是陡峭的；也就是说方向导数越大，函数在该方向的变化越快。



方向导数的定义为从给定点$P_0$发出一条射线$l$，若射线上任意点$P$逼近定点$P_0$ 时极限都存在：
$$
\lim_{p\rightarrow0^+}= \frac{f(P)-f(P_0)}{p}=\lim_{p\rightarrow0^+}\frac{\Delta f}{p}
$$
则称这个极限为函数$f$在点$P_0$沿方向$l$的方向导数，记为$f_l(P_0)$ ， 其中$p$是两点间的距离。

★   一、方向导数最后是一个数（极限的值）；二、任意方向的向量（上述的射线）自然考虑可以用空间的基向量（basis vector）表示；由此来考虑下面的定理。



该定理为：(存在方向导数的充分不必要条件)

若函数$f$在该点$P_0$ 可微，则$f$在点$P_0$沿着任一方向$l$的方向导数都存在，为：
$$
f_l(P_0) = \nabla f\cdot \vec v
$$


⌧ 其中$\vec v=\sum_{i}\alpha_ie_i$ , $\sum_i\alpha_i^2=1$为任意方向上的向量, 作为向量空间的基向量$e_i$的表示。



★   下面给出梯度与方向导数的关系，然后说明结论；



梯度的定义：若$f$在点$P_0$存在对所有自变量的偏导数，那么梯度为：
$$
\nabla f =(f_x(x_0),f_y(y_0),\cdots,f_z(z_0))
$$
任意方向方向导数等于:
$$
f_l(P_0) = \nabla f\cdot \vec v =\Vert\nabla f\Vert \Vert\vec v\Vert \cos \theta=\Vert\nabla f\Vert\cos \theta
$$
★   一、由线性代数可知，当点积达到最大值，两个向量指向的方向一致。这意味着任意向量$\vec v$与梯度方向一致的时候，方向导数取得最大，说明梯度是对应于最陡峭的上升/下降速率；二、方向导数是一个值，每一个出发点$P_0$在任意方向都有这么一个值，而该值最大的时候说明对应的方向上函数$f$取得最大的变化率。

定理：方向导数最大值为梯度$||\nabla f||$,当且仅当该方向为梯度向量的方向$\nabla f(\vec x)$。





例题是：假设我处于一个高于海拔的山坡的$\left( {60,100} \right)$位置上，上坡的高由函数$z = 1000 - 0.01{x^2} - 0.02{y^2}$给出，那么海拔变化最快的方向是?我站的地方海拔的最大变化率是多少?

答1:方向便是梯度方向：$\nabla f\left( {60,100} \right) = \left\langle { - 1.2, - 4} \right\rangle$

答2:最大变化率便是梯度导数在梯度方向的大小，等于梯度向量的范数：$\left\| {\nabla f\left( {60,100} \right)} \right\| = 4.176$.





---------



时间：20.06.16

#### Batch Gradient Descent（即Gradient Descent）

做法：在每次用梯度更新参数时，**总是计算全部训练集里参数$\Theta$的梯度**，然后做随机梯度下降更新模型参数。

注意：避免特征尺度（Scale）影响梯度下降的过程，一般所有特征的尺度做标准化。`StantardScaler` in Sklearn。

缺点：慢；陷入局部最优。



```python
eta = 0.1  # learning rate
n_iterations = 1000



m = 100

theta = np.random.randn(2,1)  # random initialization

for iteration in range(n_iterations):
    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - eta * gradients
```



#### Stochastic Gradient Descent

做法：每一个轮次**只使用一个样本进行梯度计算并更新**。有一个learning schedule，目的是迭代后期降低学习率，使得稳定在全局最优处。

优点：has a better chance of finding the global minimum than BGD does.

缺点：随机性太大因此跳出全局最优，设置学习表(神似模拟退火)



```python
theta_path_mgd = []

n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2,1)  # random initialization

t0, t1 = 200, 1000
def learning_schedule(t):
    return t0 / (t + t1)

t = 0
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minibatch_size):
        t += 1
        xi = X_b_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(t)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)
```



#### Mini-Batch Gradient Descent

做法：computes the gradients on small random sets of instances called mini-batches.

```python
theta_path_mgd = []

n_iterations = 50
minibatch_size = 20

np.random.seed(42)
theta = np.random.randn(2,1)  # random initialization

t0, t1 = 200, 1000
def learning_schedule(t):
    return t0 / (t + t1)

t = 0
for epoch in range(n_iterations):
    shuffled_indices = np.random.permutation(m)
    X_b_shuffled = X_b[shuffled_indices]
    y_shuffled = y[shuffled_indices]
    for i in range(0, m, minibatch_size):
        t += 1
        xi = X_b_shuffled[i:i+minibatch_size]
        yi = y_shuffled[i:i+minibatch_size]
        gradients = 2/minibatch_size * xi.T.dot(xi.dot(theta) - yi)
        eta = learning_schedule(t)
        theta = theta - eta * gradients
        theta_path_mgd.append(theta)
```







#### 

-----

* Reference: 华东师范《数学分析》
* https://math.stackexchange.com/questions/223252/why-is-gradient-the-direction-of-steepest-ascent



